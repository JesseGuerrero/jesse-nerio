<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Guide to Language Models: The Perfect Balance Between Power and Price</title>
    <style>
        body {
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
        }
        header {
            border-bottom: 2px solid #eee;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }
        h1 {
            margin: 0 0 10px 0;
            font-size: 2em;
        }
        .meta {
            color: #666;
            font-size: 0.9em;
        }
        .content {
            margin-bottom: 40px;
        }
        .back-link {
            display: inline-block;
            margin-top: 20px;
            color: #0066cc;
            text-decoration: none;
        }
        .back-link:hover {
            text-decoration: underline;
        }
        nav {
            background: #f8f8f8;
            padding: 10px 0;
            margin: -20px -20px 20px -20px;
            border-bottom: 1px solid #ddd;
        }
        nav ul {
            list-style: none;
            padding: 0;
            margin: 0;
            display: flex;
            gap: 20px;
            padding: 0 20px;
        }
        nav a {
            color: #333;
            text-decoration: none;
        }
        nav a:hover {
            color: #0066cc;
        }
        .content img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
        }
        h2 {
            margin-top: 30px;
            color: #222;
        }
    </style>
</head>
<body>
    <nav>
        <ul>
            <li><a href="https://jessenerio.com">Home</a></li>
            <li><a href="https://jessenerio.com/publications">Publications</a></li>
            <li><a href="../../portfolio/">Portfolio</a></li>
            <li><a href="../index.html">Blog Posts</a></li>
        </ul>
    </nav>
    <header>
        <h1>A Guide to Language Models: The Perfect Balance Between Power and Price</h1>
        <div class="meta"></div>
    </header>

    <article class="content">
        <p>Your first and foremost goal should be to choose a path.</p>

        <p>You have either the high performant path or the lower performant path.</p>

        <p>The issue is power, accuracy and message length directly scale with parameter count and GPU power.</p>

        <p>Your goal in customizing a language model should be a balance between power and cost.</p>

        <p>Only you know your needs and circumstances, I will just show you the options that are out there for language models.</p>

        <p>For this, it doesn't matter how you plan to train, customize or use language models, it will cost money.</p>

        <img src="../images/language-models-guide-img-0.jpeg" alt="NVIDIA GPU">

        <p>Perhaps the only free thing you can do is use a language model that was already free. Even if you are solely a user of language models, paying for premium will immensely improve your results from the model just because of GPU power costs.</p>

        <h2>Requirements</h2>

        <p>To even use a custom model you need some efficient way to use GPU power.</p>

        <p>It doesn't matter if it is a remote GPU for rent or a GPU at home you buy. There are libraries for both remote and at-home.</p>

        <p>The key difference is price.</p>

        <h2>Pricing For Custom High Performance models</h2>

        <p>I would say a high performing model is at minimum 25B+ parameters.</p>

        <img src="../images/language-models-guide-img-1.png" alt="HuggingFace models list">

        <p>If you want true performance 40B+ is where its at.</p>

        <p>You will be surprised how well these high parameter models perform.</p>

        <p>But yea, its expensive.</p>

        <p>For a high performing model you will have to spend money, there is no getting around it</p>

        <p>Let's look into it a little bit.</p>

        <p>Contabo is a known remote PC vendor for its cheapest pricing with remarkable features.</p>

        <p>Take a look at their pricing...</p>

        <img src="../images/language-models-guide-img-2.png" alt="Contabo GPU pricing">

        <p>This is cheaper than Amazon and the grand majority of GPU vendors out there.</p>

        <p>Notice the extravagant prices with 24GB of VRAM at $312/month.</p>

        <p>If you wanted to create your own high performing LLM with a remote server, these would be the best costs.</p>

        <p>Now lets look at GPU prices here online.</p>

        <img src="../images/language-models-guide-img-3.png" alt="NVIDIA Tesla A40 on Amazon">

        <p>That same GPU at twice the VRAM costs upwards of $4,000+.</p>

        <p>These are the prices you should be thinking for high performance and customization, <em>at the minimum</em>.</p>

        <p>70B+ parameter models are where you get competitive with production models.</p>

        <p>These often take 40GB+ VRAM to host.</p>

        <p>Sad, but true.</p>

        <p>The prices of hardware will go down, models are going to get smaller and AI specific hardware is being created as we speak.</p>

        <p>Give it time, but for now language modeling is a gold rush.</p>

        <h2>Pricing For Lower Performance Models</h2>

        <p>Thankfully we can use lower performing models for much cheaper.</p>

        <p>I will be transparent, anything below about 7B parameters is not worth it for your own usage.</p>

        <p>0.5B to 4B parameters would be good for testing, developing models and fine-tuning but for actual usage the recommended minimum is 7B parameters.</p>

        <p>The model itself impacts performance, but the higher parameters are the starting point.</p>

        <p>For lower models I would say you need at least 12GB of VRAM.</p>

        <p>Ideally you would have around 20GB of VRAM but 12 will get you by running a 7B parameter model.</p>

        <p>At these levels I would not recommend renting a GPU but rather buying a GPU at home.</p>

        <p>Let's look at the price:</p>

        <img src="../images/language-models-guide-img-4.png" alt="MSI RTX 3060 12GB on Amazon">

        <p>For a 12GB card you are looking at around $250-$280.</p>

        <p>Much more accessible.</p>

        <p>These are worth it for general programming and development of graphics/AI at home.</p>

        <p>You can run a model with this using model compression.</p>

        <p>Though I would still recommend shilling out another $200 for the extra 4GB of VRAM</p>

        <img src="../images/language-models-guide-img-5.png" alt="PNY RTX 4060 Ti 16GB">

        <p>At 12GB you would have to exit all GPU intensive programs and dedicate the entire GPU to the AI.</p>

        <p>Just those 4GB will massively improve model performance and give you a little wiggle room to run a video game or GPU programs in the background.</p>

        <h2>Methodology for saving money</h2>

        <p>Where you need 60GB-80GB of VRAM to train/fine-tune you only need about 30GB to actually host the model.</p>

        <p>For this reason I recommend <em>always always always</em> renting a high performance rack for fine-tuning and training.</p>

        <p>If you train on your own GPU, your requirements sky rocket for the same model power.</p>

        <p><em>It takes much less GPU power/VRAM to use a model</em> rather than train or fine-tune a model.</p>

        <p>You will save yourself a lot of hassle training remote.</p>

        <p>Here is a vendor price for this:</p>

        <img src="../images/language-models-guide-img-6.png" alt="vast.ai GPU rental pricing">

        <p>Look at those wonderful prices. Mwah!</p>

        <p><a href="https://vast.ai/">vast.ai</a> at this time provides the cheapest model renting per hour on the space.</p>

        <p>Yes if you multiply the usage/hour to an entire month it is extremely expensive.</p>

        <p>But, you only need the model running for 20-40 hours or so, depending on your dataset.</p>

        <p>The basic idea is to get a low parameter model like 0.5B or 1.3B and train/tune it on your custom dataset.</p>

        <p>Because of how the transformers library works all you need to do is replace the name of the model in the script and it is ready for training,</p>

        <img src="../images/language-models-guide-img-7.png" alt="Model name replacement code">

        <p>The basic steps for using a remote training GPU is this:</p>

        <ol>
            <li>At a low parameter count create a fine-tune or training script</li>
            <li>Replace the model name with a high parameter count</li>
            <li>Convert the script to a python file</li>
            <li>Run and tune the script on the remote server</li>
            <li>Upload the new model to HuggingFace</li>
            <li>Delete and remove the GPU server</li>
            <li>Use your custom LLM</li>
        </ol>

        <p>It all starts with pricing, and those points I just made are your options.</p>

        <h2>Why the transformers library for beginners</h2>

        <p>Lets say you have chosen your path and model parameter count. How would I get started learning language models?</p>

        <p>Well, you don't need much.</p>

        <p>In reality you just need some <s>python skills</s>, I mean Chat-GPT premium.</p>

        <img src="../images/language-models-guide-img-8.png" alt="Transformers library code example">

        <p>The HuggingFace transformers library has abstracted away so much of machine learning, you don't need some massive PhD to get started.</p>

        <p>All you need is a working script you can find on google, replace the model name and copy paste into Chat-GPT to explain things.</p>

        <p>I know "use Chat-GPT" won't age well, but if its 2026+ when you are reading this, just replace Chat-GPT with the best language model vendor of your time.</p>

        <p>These abstraction libraries give immense power for little skill.</p>

        <p>After some time using the transformers library you can switch to pure PyTorch or start getting more granular on how you train models.</p>

        <h2>Just use a custom model</h2>

        <p>The first and foremost thing is to simply learn to use different models with the transformers library.</p>

        <img src="../images/language-models-guide-img-9.png" alt="Google Colab fine-tuning notebook">

        <p>Start up a Python notebook and run a model.</p>

        <p>Try to get it hosted with Gradio inside the script or run it on Ollama with some video card frontend.</p>

        <p>You can find <a href="https://github.com/ollama/ollama">Ollama here</a>.</p>

        <p>The basic idea is to simply start consuming custom models.</p>

        <p>I would recommend Ollama and Open Web UI:</p>

        <p>Backend: <a href="https://github.com/ollama/ollama">Ollama</a></p>

        <p>Frontend: <a href="https://github.com/open-webui/open-webui">Open Web UI</a></p>

        <p>With these two you can insert your custom models, tuned and trained into Ollama and deliver the model to Open Web UI for a look like this:</p>

        <img src="../images/language-models-guide-img-10.png" alt="Open Web UI interface">

        <p>Once you have the workflow for simply using custom models, you can move on to training and fine-tuning.</p>

        <h2>Content to consume</h2>

        <p>There is something I want to explain about content regarding deep learning.</p>

        <p>Its the math. The math is all the same.</p>

        <p>What we are looking at is abstractions.</p>

        <p>All the changes and amazing libraries, methodologies and framework features, its great and all but <strong>the underlying math is all the same</strong>.</p>

        <p>It doesn't matter how many years pass, deep learning is deep learning.</p>

        <p>For this reason, don't be afraid to study older deep learning material.</p>

        <p>There is a lot of evergreen machine learning content out there.</p>

        <p>The issue is identifying which are evergreen and which are outdated.</p>

        <img src="../images/language-models-guide-img-11.jpeg" alt="UC Davis College historical photo">

        <p>The math for deep learning has been around since before the 1960s. So why have we not developed AI till now?</p>

        <p>Hardware.</p>

        <p>That is the answer its hardware.</p>

        <p>We are creating all these abstraction layers upon layers on the fundamentals of machine learning.</p>

        <p>Yes it is very good to learn the layers.</p>

        <p>And yes its good to start at the top of the layer, the most abstract, and work your way down.</p>

        <p>But much machine learning content from years ago is still essential for overall learning.</p>

        <p>It is still relevant.</p>

        <p><strong>What is sort of useless is the old abstractions.</strong></p>

        <p><strong>The super low level stuff will always be the same</strong>, its the outdated abstractions that you don't want.</p>

        <p>Despite this it is good to start at the highest layer, the most abstract for developing workflows and general understanding.</p>

        <p>Assuming you have chosen your pricing path, let's look at the easiest resources to consume.</p>

        <p>If you are an absolute beginner, I recommend consuming these from top to bottom</p>

        <ol>
            <li>At this time the best blog and working Python script I could find was this one. I recommend starting with the script: <a href="https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html">Fine-Tune Your Own Model Using Transformers Library</a></li>
            <li>Next up is a <a href="https://youtu.be/mrKuDK9dGlg?si=WoOZv3cNfqraa5Fz">lecture on LLMs for beginners</a> where he explains the latest abstractions</li>
            <li>The best explanation for the lower layers (evergreen): <a href="https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=iv8Fbt2BVL5PbIap">deep learning under the hood</a></li>
            <li>Read (evergreen) <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a></li>
            <li>Read (evergreen) <a href="https://arxiv.org/abs/1810.04805">Bert Transformers</a></li>
            <li>Read (evergreen) <a href="https://arxiv.org/abs/1301.3781">Word2Vec</a></li>
            <li>Code, read daily</li>
        </ol>

        <p>Anywho, I hope you learned something...</p>

        <p>CTA: Check out my <a href="https://a.co/d/b56kpK9">book on learning code</a></p>

        <p>Happy coding!</p>

        <h2>Resources</h2>

        <p>Rent GPUs for tuning/training: <a href="https://vast.ai/">https://vast.ai/</a></p>

        <p>Backend for model hosting: <a href="https://github.com/ollama/ollama">https://github.com/ollama/ollama</a></p>

        <p>Frontend for model hosting: <a href="https://github.com/open-webui/open-webui">https://github.com/open-webui/open-webui</a></p>

        <p>Free beginner Python script: <a href="https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html">https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html</a></p>

        <p>Great LLM lecture: <a href="https://youtu.be/mrKuDK9dGlg?si=WoOZv3cNfqraa5Fz">https://youtu.be/mrKuDK9dGlg?si=WoOZv3cNfqraa5Fz</a></p>

        <p>Great deep learning playlist: <a href="https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=iv8Fbt2BVL5PbIap">https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=iv8Fbt2BVL5PbIap</a></p>

        <p>Attention is all you need: <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></p>

        <p>Bert: <a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></p>

        <p>Word2Vec: <a href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</a></p>
    </article>

    <footer>
        <a href="../index.html" class="back-link">&larr; Back to Blog</a>
    </footer>
</body>
</html>
